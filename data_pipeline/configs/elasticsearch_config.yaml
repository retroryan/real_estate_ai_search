# Elasticsearch Pipeline Configuration
# This configuration file is for Elasticsearch output from the Spark data pipeline
# Use this with: python -m data_pipeline --config elasticsearch_config.yaml

# Pipeline metadata
metadata:
  name: "elasticsearch_data_pipeline"
  version: "1.0.0"
  description: "Apache Spark pipeline with Elasticsearch output for real estate and Wikipedia data"

# Spark configuration
spark:
  app_name: "ElasticsearchDataPipeline"
  master: "local[*]"
  memory: "4g"
  executor_memory: "2g"
  config:
    # Add Elasticsearch Spark connector JAR
    spark.jars.packages: "org.elasticsearch:elasticsearch-spark-30_2.12:8.11.4"
    spark.sql.adaptive.enabled: "true"
    spark.sql.adaptive.coalescePartitions.enabled: "true"

# Data processing configuration
  enabled: false  # Set to true for testing with smaller datasets
  sample_size: 100
  properties_limit: 50
  neighborhoods_limit: 20
  wikipedia_limit: 30

# Data sources
data_sources:
  properties:
    path: "real_estate_data/properties_sf.json"
    format: "json"
    enabled: true
    options:
      multiLine: true
  
  neighborhoods:
    path: "real_estate_data/neighborhoods_sf.json"
    format: "json"
    enabled: true
    options:
      multiLine: true
  
  wikipedia:
    path: "data/wikipedia/wikipedia.db"
    format: "sqlite"
    enabled: true
    options:
      url: "jdbc:sqlite:data/wikipedia/wikipedia.db"
      dbtable: "articles"
      driver: "org.sqlite.JDBC"

# Data enrichment configuration
enrichment:
  normalize_features: true
  deduplicate_features: true
  sort_features: true
  add_derived_fields: true
  calculate_price_per_sqft: true
  validate_addresses: true
  city_abbreviations:
    SF: "San Francisco"
    PC: "Park City"
    LA: "Los Angeles"
    NYC: "New York City"
  state_abbreviations:
    CA: "California"
    UT: "Utah"
    NY: "New York"

# Text processing and chunking
chunking:
  enabled: true
  method: "simple"
  chunk_size: 512
  chunk_overlap: 50
  respect_sentence_boundaries: true

# Embedding configuration (if needed for search)
embedding:
  provider: "mock"  # Use mock for demo, can be ollama/openai/voyage/gemini
  batch_size: 100
  skip_existing: false

# Multi-destination output configuration
output_destinations:
  enabled_destinations:
    - "elasticsearch"
    - "parquet"  # Also save to Parquet for backup/analysis
  
  # Elasticsearch configuration
  elasticsearch:
    enabled: true
    hosts:
      - "localhost:9200"
    # Authentication (use environment variables for production)
    # username: "elastic"
    # password: "${ES_PASSWORD}"  # Set ES_PASSWORD environment variable
    index_prefix: "realestate"
    bulk_size: 500
    clear_before_write: true  # Demo mode - clear indices before writing
  
  # Parquet configuration (for backup)
  parquet:
    enabled: true
    path: "data/processed/elasticsearch_pipeline"
    partitioning_columns:
      - "entity_type"
    compression: "snappy"
    mode: "overwrite"

# Standard output configuration (fallback)
output:
  format: "parquet"
  path: "data/processed/elasticsearch_pipeline_fallback"
  partitioning:
    enabled: true
    columns:
      - "entity_type"
      - "state"
  compression: "snappy"
  overwrite: true

# Logging configuration
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
  console: true
  file: "logs/elasticsearch_pipeline.log"
  log_execution_time: true
  log_record_counts: true