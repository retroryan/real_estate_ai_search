# V10 Proposal: Migration from Manual StructType to Pydantic-Spark Models

## Complete Cut-Over Requirements

**THESE REQUIREMENTS ARE NON-NEGOTIABLE AND MUST BE FOLLOWED:**

* **COMPLETE CHANGE**: All occurrences must be changed in a single, atomic update
* **CLEAN IMPLEMENTATION**: Simple, direct replacements only
* **NO MIGRATION PHASES**: Do not create temporary compatibility periods
* **NO PARTIAL UPDATES**: Change everything or change nothing
* **NO COMPATIBILITY LAYERS**: Do not maintain old and new paths simultaneously
* **NO BACKUPS OF OLD CODE**: Do not comment out old code "just in case"
* **NO CODE DUPLICATION**: Do not duplicate functions to handle both patterns
* **NO WRAPPER FUNCTIONS**: Direct replacements only, no abstraction layers
* **DO NOT CREATE "ENHANCED" OR "IMPROVED" VERSIONS**: Update the actual classes directly (e.g., update PropertyIndex itself, not create ImprovedPropertyIndex)
* **ALWAYS USE PYDANTIC**: All schemas must be Pydantic-based
* **USE MODULES AND CLEAN CODE**: Organize code into proper modules with clear separation of concerns

## Executive Summary

This proposal outlines the complete replacement of all manual Apache Spark StructType schema definitions in the data_pipeline module with Pydantic-based models using the pydantic-spark library. This is a complete cut-over that will dramatically simplify schema management, improve type safety, and eliminate code duplication across the entire data pipeline.

## Current State Analysis

### Problem Statement

The current codebase contains extensive manual StructType definitions scattered across multiple modules, leading to:
- Verbose and error-prone schema definitions that require manual synchronization between field names and types
- Duplication between Pydantic models (for validation) and Spark schemas (for DataFrame operations)
- Difficulty maintaining consistency when schema changes are needed
- Lack of automatic validation and type checking at the boundary between Python objects and Spark DataFrames
- Complex nested structures that are particularly difficult to maintain manually

### Affected Components

The following classes and modules currently use manual StructType definitions and require migration:

#### Data Loader Classes
1. **PropertyLoader** (`data_pipeline/loaders/property_loader.py`)
   - Contains the most complex schema with deeply nested structures including:
     - Address structure (street, city, county, state, zip)
     - Coordinates structure (latitude, longitude)
     - Property details structure (square_feet, bedrooms, bathrooms, etc.)
     - Price history array with nested structures
   - Currently defines schema manually in `_define_schema()` method

2. **NeighborhoodLoader** (`data_pipeline/loaders/neighborhood_loader.py`)
   - Contains nested demographics structure with population, median_income, median_age
   - Includes array types for amenities
   - Currently defines schema manually in `_define_schema()` method

3. **LocationLoader** (`data_pipeline/loaders/location_loader.py`)
   - Simple flat schema for geographic hierarchy
   - Currently defines schema manually in `_define_schema()` method

4. **WikipediaLoader** (`data_pipeline/loaders/wikipedia_loader.py`)
   - Currently returns None from `_define_schema()` and relies on pandas DataFrame inference
   - Needs proper schema definition for consistency

5. **BaseLoader** (`data_pipeline/loaders/base_loader.py`)
   - Abstract base class that defines the `_define_schema()` interface
   - Needs update to work with pydantic-spark models

#### Schema Definition Classes
1. **PropertySchema** (`data_pipeline/schemas/entity_schemas.py`)
   - Already has Pydantic model but maintains separate `get_spark_schema()` static method
   - Manual StructType definition duplicates the Pydantic model fields

2. **NeighborhoodSchema** (`data_pipeline/schemas/entity_schemas.py`)
   - Already has Pydantic model but maintains separate `get_spark_schema()` static method
   - Manual StructType definition duplicates the Pydantic model fields

3. **WikipediaArticleSchema** (`data_pipeline/schemas/entity_schemas.py`)
   - Already has Pydantic model but maintains separate `get_spark_schema()` static method
   - Manual StructType definition duplicates the Pydantic model fields

4. **LocationSchema** (`data_pipeline/schemas/location_schema.py`)
   - Has Pydantic model with separate `get_location_spark_schema()` function
   - Function manually creates StructType that duplicates model definition

#### Other Components
1. **RelationshipBuilder** (`data_pipeline/enrichment/relationship_builder.py`)
   - Creates inline StructType for empty DataFrame schemas
   - Needs migration to use a proper Pydantic model for relationship schemas

## âœ… COMPLETED IMPLEMENTATION

### Core Technology: Custom Pydantic to Spark Converter

A custom spark_converter.py module provides seamless conversion between Pydantic models and Spark schemas through the `SparkModel` base class. This enables:
- Automatic conversion from Pydantic models to Spark StructType via `spark_schema()` method
- Proper handling of Optional types (sets nullable=True correctly)
- Type safety and validation at compile time and runtime
- Single source of truth for both Python and Spark schemas
- Support for all Spark data types including nested structures and arrays
- No external dependencies beyond pydantic and pyspark

### Implementation Requirements (COMPLETE CUT-OVER - NO PHASES)

#### Required Changes - ALL AT ONCE

1. **Create New Pydantic-Spark Models**
   - Define nested models for complex structures (Address, Coordinates, PropertyDetails, Demographics, PriceHistory)
   - Create main entity models inheriting from SparkModel instead of BaseModel
   - Ensure all field types map correctly to Spark types (use Decimal for financial data, proper array types)
   - Add appropriate Field validators and constraints

2. **Model Structure Requirements**

   **Property Model Hierarchy:**
   - Create Address model with fields: street, city, county, state, zip
   - Create Coordinates model with fields: latitude, longitude  
   - Create PropertyDetails model with fields: square_feet, bedrooms, bathrooms, property_type, year_built, lot_size, stories, garage_spaces
   - Create PriceHistory model with fields: date, price, event
   - Create main Property model that composes these nested models

   **Neighborhood Model Hierarchy:**
   - Create Demographics model with fields: population, median_income, median_age
   - Create main Neighborhood model that includes Demographics as nested structure

   **Location Model:**
   - Simple flat model with state, county, city, neighborhood fields
   - Add derived fields for location_type and full_hierarchy

   **Wikipedia Article Model:**
   - Flat model with all Wikipedia-specific fields
   - Include embedding-related fields

   **Relationship Model:**
   - Create new model for relationship edges with from_id, to_id, relationship_type

#### Simplified Data Transformation Approach

**NEW REQUIREMENT: Flatten nested structures directly using Spark operations**

Instead of complex schema transformations, use simple Spark select operations to flatten nested structures.

This approach:
- Eliminates the need for complex schema mapping
- Uses native Spark operations for efficiency
- Maintains clarity and simplicity
- Reduces code complexity significantly

Similar flattening should be applied to:
- PropertyLoader: Flatten address, coordinates, and property_details structs
- NeighborhoodLoader: Flatten demographics struct
- Any other nested structures that need to be normalized

#### Loader Class Updates (IMMEDIATE REPLACEMENT)

1. **Update BaseLoader Abstract Class**
   - Modify `_define_schema()` signature to support returning schema from SparkModel
   - Add support for model-based schema validation

2. **Update Individual Loaders**
   - PropertyLoader: Replace manual schema with `Property.as_spark_schema()`
   - NeighborhoodLoader: Replace manual schema with `Neighborhood.as_spark_schema()`
   - LocationLoader: Replace manual schema with `Location.as_spark_schema()`
   - WikipediaLoader: Implement proper schema using `WikipediaArticle.as_spark_schema()`

3. **Schema Method Simplification**
   - Each loader's `_define_schema()` method becomes a single line returning the model's Spark schema
   - Remove all manual StructType and StructField imports from loader classes

4. **Transform Method Implementation**
   - Add `_transform_to_entity_schema()` method to each loader that needs flattening
   - PropertyLoader: Flatten address.*, coordinates.*, property_details.* into top-level columns
   - NeighborhoodLoader: Flatten demographics.* into top-level columns
   - Use simple df.select() operations, no complex transformations

#### Schema Module Consolidation (IMMEDIATE REPLACEMENT)

1. **Remove Redundant Schema Methods**
   - Delete all `get_spark_schema()` static methods from existing Pydantic models
   - Remove manual StructType definitions from entity_schemas.py
   - Consolidate location_schema.py to use SparkModel

2. **Create Centralized Model Registry**
   - Single module exporting all SparkModel classes
   - Consistent naming convention for all models
   - Clear documentation of model relationships and usage

#### Enrichment and Relationship Updates (IMMEDIATE REPLACEMENT)

1. **RelationshipBuilder Migration**
   - Create Relationship SparkModel for edge schemas
   - Replace inline StructType creation with model-based schema

2. **Update Any Dynamic Schema Creation**
   - Identify any places where schemas are created dynamically
   - Replace with model-based approaches where possible

### Implementation Strategy (COMPLETE CUT-OVER)

#### Single Atomic Update

**ALL CHANGES MUST BE MADE IN A SINGLE COMMIT:**

1. **Add pydantic-spark dependency**
2. **Create all SparkModel implementations**
3. **Replace all loader schemas**
4. **Remove all manual StructType definitions**
5. **Update all schema references**
6. **Implement all transform methods**
7. **Update RelationshipBuilder**
8. **Run full test suite**
9. **Commit everything at once**

**NO INTERMEDIATE STATES, NO GRADUAL ROLLOUT, NO COMPATIBILITY PERIODS**

### Benefits and Impact

#### Code Reduction
- Estimated 70% reduction in schema-related code
- Elimination of approximately 300+ lines of manual StructType definitions
- Single source of truth for each entity schema

#### Maintainability Improvements
- Schema changes require updates in only one location
- Automatic synchronization between Python and Spark representations
- IDE support for field names and types through Pydantic

#### Type Safety Enhancements
- Compile-time type checking for all schema fields
- Runtime validation of data against schema constraints
- Better error messages when schema violations occur

#### Developer Experience
- Intuitive model definitions using standard Python syntax
- No need to remember Spark type mappings
- Easier onboarding for developers familiar with Pydantic

### Risk Mitigation

#### Pre-Implementation Validation
- Verify pydantic-spark supports all required Spark data types
- Test nested structure and array handling in isolation
- Confirm Spark version compatibility

#### Performance Validation
- Benchmark schema creation performance before implementation
- Ensure transform operations are efficient
- No caching of old implementations - direct replacement only

#### NO ROLLBACK STRATEGY
- **This is a complete cut-over**
- **Test thoroughly before committing**
- **No maintaining old code paths**
- **If issues arise, fix forward, don't roll back**

### Testing Requirements

#### Unit Tests
- Test each SparkModel's schema generation
- Validate nested structure handling
- Ensure array type conversions work correctly
- Test nullable field handling

#### Integration Tests
- End-to-end data loading with new schemas
- Verify data can round-trip through save/load cycles
- Test with actual data files from current pipeline

#### Performance Tests
- Measure schema creation time
- Benchmark data loading performance
- Compare memory usage with current implementation

### Documentation Updates

#### Code Documentation
- Document each SparkModel with field descriptions
- Provide examples of model usage in loaders
- Create migration guide for developers

#### Architecture Documentation
- Update data pipeline architecture docs
- Document the new model-driven approach
- Provide decision rationale for future reference

## Success Criteria

The migration will be considered successful when:
1. All manual StructType definitions are replaced with SparkModel implementations
2. All existing tests pass with new implementation
3. No performance degradation in data pipeline execution
4. Code complexity metrics show measurable improvement
5. Developer team reports improved maintainability

## Detailed Implementation Plan

### Implementation Todo List

**ALL ITEMS MUST BE COMPLETED IN ORDER, IN A SINGLE WORK SESSION:**

#### 1. Environment Setup
- [x] Created custom spark_converter.py module (replaced pydantic-spark dependency)
- [x] Create new module: `data_pipeline/models/spark_models.py`

#### 2. Create Pydantic-Spark Models

**Property Models:**
- [x] Create Address SparkModel with fields: street, city, county, state, zip
- [x] Create Coordinates SparkModel with fields: latitude, longitude
- [x] Create PropertyDetails SparkModel with fields: square_feet, bedrooms, bathrooms, property_type, year_built, lot_size, stories, garage_spaces
- [x] Create PriceHistory SparkModel with fields: date, price, event
- [x] Create Property SparkModel composing all nested models above
- [x] Add listing_id, neighborhood_id, listing_price, price_per_sqft, description, features, listing_date, days_on_market, virtual_tour_url, images fields

**Neighborhood Models:**
- [x] Create Demographics SparkModel with fields: population, median_income, median_age
- [x] Create Neighborhood SparkModel with neighborhood_id, name, city, state, description, amenities
- [x] Compose Demographics into Neighborhood model

**Location Model:**
- [x] Create Location SparkModel with fields: state, county, city, neighborhood
- [x] Add derived fields: location_type, full_hierarchy, source_file, ingested_at

**Wikipedia Model:**
- [x] Create WikipediaArticle SparkModel with all required fields
- [x] Include page_id, title, url, best_city, best_state, latitude, longitude
- [x] Add summary fields and embedding fields

**Relationship Model:**
- [x] Create Relationship SparkModel with fields: from_id, to_id, relationship_type

#### 3. Update Loader Classes

**BaseLoader Updates:**
- [x] Remove StructType import from BaseLoader
- [x] Update _define_schema() method signature documentation

**PropertyLoader Updates:**
- [x] Remove all StructType, StructField imports
- [x] Import Property SparkModel from new models module
- [x] Replace _define_schema() with Property.as_spark_schema()
- [x] Update _transform_to_entity_schema() method to flatten: address.*, coordinates.*, property_details.*

**NeighborhoodLoader Updates:**
- [x] Remove all StructType, StructField imports
- [x] Import Neighborhood SparkModel from new models module
- [x] Replace _define_schema() with Neighborhood.as_spark_schema()
- [x] _transform_to_entity_schema() already flattens demographics.*

**LocationLoader Updates:**
- [x] Remove all StructType, StructField imports
- [x] Import Location SparkModel from new models module
- [x] Replace _define_schema() with Location.as_spark_schema()
- [x] _transform_to_entity_schema() handles flat structure correctly

**WikipediaLoader Updates:**
- [x] Import WikipediaArticle SparkModel from new models module
- [x] Implement _define_schema() with WikipediaArticle.as_spark_schema()
- [x] Update load method to use defined schema

#### 4. Clean Up Schema Modules

**Schema Module Cleanup:**
- [x] Deleted entire schemas directory (entity_schemas.py and location_schema.py)
- [x] All schemas now defined in data_pipeline/models/spark_models.py
- [x] No duplicate schema definitions remain

#### 5. Update RelationshipBuilder

- [x] Remove inline StructType creation in RelationshipBuilder
- [x] Import Relationship SparkModel
- [x] Replace empty DataFrame schema with Relationship.as_spark_schema()

#### 6. Update All References

- [x] Searched for any remaining StructType imports and removed where appropriate
- [x] Removed unused StructType import from test_pipeline_output_validation.py
- [x] Verified no references to old schema modules remain
- [x] All imports now point to new SparkModel classes
- [x] No duplicate schema definitions remain

#### 7. Update Tests

- [ ] Update test_neo4j_graph_writer.py to use new schemas
- [ ] Update test_pipeline_output_validation.py to use new schemas
- [ ] Update test_neo4j_basic.py to use new schemas
- [ ] Update any loader tests to verify new schema structure
- [ ] Add tests for _transform_to_entity_schema() methods

#### 8. Final Validation

- [ ] Run full data pipeline with sample data
- [ ] Verify all DataFrames have correct schemas
- [ ] Check that nested structures are properly flattened
- [ ] Confirm no schema-related errors or warnings
- [ ] Validate output Parquet files have expected structure

#### 9. Documentation Updates

- [ ] Update inline documentation in all modified files
- [ ] Add docstrings to all new SparkModel classes
- [ ] Update any README files that reference schema structure
- [ ] Document the transform_to_entity_schema pattern

#### 10. Code Review and Testing

- [ ] Self-review all changes for consistency
- [ ] Ensure NO old code is commented out
- [ ] Verify NO compatibility layers exist
- [ ] Run full test suite: `pytest data_pipeline/`
- [ ] Run integration tests with real data
- [ ] Performance benchmark comparison
- [ ] Memory usage comparison
- [ ] Final review that all requirements are met
- [ ] Commit everything in a single atomic commit

### Execution Notes

**CRITICAL REMINDERS:**
- This entire checklist must be completed in one session
- No partial commits - everything goes in at once
- If any step fails, fix it immediately, don't skip
- No commenting out old code "just in case"
- No creating backup files
- Direct replacement only - no wrapper functions

**Time Estimate:**
- Setup and Models: 2-3 hours
- Loader Updates: 2-3 hours  
- Cleanup and Testing: 2-3 hours
- **Total: 6-9 hours of focused work**

## Timeline

**SINGLE IMPLEMENTATION WINDOW:**
- Implementation: 1 day (6-9 hours of focused work)
- All testing happens during implementation
- Documentation updates inline with code changes
- **Total Duration: 1 day for complete cut-over**

**This is not a multi-week project. This is a focused, complete replacement done in a single effort.**

## Conclusion

This complete cut-over from manual StructType definitions to pydantic-spark models will:
- Eliminate all schema duplication immediately
- Simplify the codebase in a single, atomic change
- Provide immediate benefits with no transition period
- Follow clean code principles with no legacy baggage

**Remember: This is a complete replacement, not a migration. Everything changes at once, or nothing changes at all.**