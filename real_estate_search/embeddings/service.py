"""
Query embedding service for real estate search.

Generates embeddings for natural language queries using Voyage AI,
matching the embeddings generated by data_pipeline for properties.
"""

import logging
from typing import List, Optional
from pydantic import BaseModel, Field
import time

from .models import EmbeddingConfig
from .exceptions import (
    EmbeddingServiceError,
    EmbeddingGenerationError,
    ConfigurationError
)

# Import LlamaIndex embedding provider
try:
    from llama_index.embeddings.voyageai import VoyageEmbedding
except ImportError as e:
    raise ImportError(
        "Please install llama-index-embeddings-voyageai: "
        "pip install llama-index-embeddings-voyageai"
    ) from e

logger = logging.getLogger(__name__)


class QueryEmbeddingService(BaseModel):
    """
    Service for generating query embeddings.
    
    This service generates embeddings for natural language queries using the same
    Voyage-3 model used by data_pipeline for property embeddings, ensuring compatibility
    for semantic search operations.
    
    Example:
        ```python
        config = EmbeddingConfig()  # Loads API key from environment
        service = QueryEmbeddingService(config=config)
        service.initialize()
        
        query = "modern home with mountain views"
        embedding = service.embed_query(query)
        # Returns 1024-dimensional vector for KNN search
        ```
    """
    
    config: EmbeddingConfig = Field(
        description="Embedding configuration"
    )
    
    _embed_model: Optional[VoyageEmbedding] = None
    _initialized: bool = False
    
    model_config = {
        "arbitrary_types_allowed": True
    }
    
    def initialize(self) -> None:
        """
        Initialize the embedding model.
        
        Creates the VoyageEmbedding instance for generating embeddings.
        This is done separately from __init__ to allow for lazy initialization
        and better error handling.
        
        Raises:
            ConfigurationError: If configuration is invalid
            EmbeddingServiceError: If model initialization fails
        """
        if self._initialized:
            logger.debug("Embedding service already initialized")
            return
        
        # Validate API key is present
        if not self.config.api_key:
            raise ConfigurationError(
                "VOYAGE_API_KEY is required but not found. "
                "Please set it in your .env file or environment variables."
            )
        
        try:
            logger.info(f"Initializing {self.config.get_model_identifier()}")
            
            # Create Voyage embedding model
            self._embed_model = VoyageEmbedding(
                api_key=self.config.api_key,
                model_name=self.config.model_name
            )
            
            self._initialized = True
            logger.info(f"Successfully initialized embedding service with {self.config.model_name}")
            
        except Exception as e:
            error_msg = f"Failed to initialize embedding service: {str(e)}"
            logger.error(error_msg)
            raise EmbeddingServiceError(error_msg, original_error=e)
    
    def embed_query(self, query: str) -> List[float]:
        """
        Generate embedding for a query string.
        
        Converts a natural language query into a vector embedding using Voyage-3,
        suitable for KNN search against property embeddings in Elasticsearch.
        
        Args:
            query: Natural language query text
            
        Returns:
            List of floats representing the embedding vector (1024 dimensions for voyage-3)
            
        Raises:
            EmbeddingServiceError: If service is not initialized
            EmbeddingGenerationError: If embedding generation fails
        """
        if not self._initialized or not self._embed_model:
            raise EmbeddingServiceError("Service not initialized. Call initialize() first.")
        
        if not query or not query.strip():
            raise EmbeddingGenerationError(
                query,
                "Query cannot be empty"
            )
        
        # Clean the query
        query = query.strip()
        
        # Track timing
        start_time = time.time()
        
        try:
            # Generate embedding using LlamaIndex
            logger.debug(f"Generating embedding for query: '{query[:100]}...'")
            
            # VoyageEmbedding.get_text_embedding returns a list of floats
            embedding = self._embed_model.get_text_embedding(query)
            
            # Validate embedding
            if not isinstance(embedding, list):
                raise EmbeddingGenerationError(
                    query,
                    f"Invalid embedding type: {type(embedding)}"
                )
            
            if len(embedding) != self.config.dimension:
                raise EmbeddingGenerationError(
                    query,
                    f"Embedding dimension mismatch: expected {self.config.dimension}, got {len(embedding)}"
                )
            
            elapsed = (time.time() - start_time) * 1000  # Convert to milliseconds
            logger.info(f"Generated embedding in {elapsed:.1f}ms for query: '{query[:50]}...'")
            
            return embedding
            
        except Exception as e:
            if isinstance(e, EmbeddingGenerationError):
                raise
            
            error_msg = f"Failed to generate embedding: {str(e)}"
            logger.error(error_msg)
            raise EmbeddingGenerationError(query, error_msg, original_error=e)
    
    def batch_embed_queries(self, queries: List[str]) -> List[List[float]]:
        """
        Generate embeddings for multiple queries.
        
        Batch processing for multiple queries. Useful for pre-computing embeddings
        or processing multiple search queries efficiently.
        
        Args:
            queries: List of natural language query texts
            
        Returns:
            List of embedding vectors, one for each query
            
        Raises:
            EmbeddingServiceError: If service is not initialized
            EmbeddingGenerationError: If any embedding generation fails
        """
        if not self._initialized or not self._embed_model:
            raise EmbeddingServiceError("Service not initialized. Call initialize() first.")
        
        if not queries:
            return []
        
        # Clean queries
        cleaned_queries = [q.strip() for q in queries if q and q.strip()]
        
        if not cleaned_queries:
            raise EmbeddingGenerationError(
                "",
                "No valid queries provided"
            )
        
        start_time = time.time()
        
        try:
            logger.debug(f"Generating embeddings for {len(cleaned_queries)} queries")
            
            # VoyageEmbedding.get_text_embedding_batch returns a list of embedding lists
            embeddings = self._embed_model.get_text_embedding_batch(cleaned_queries)
            
            # Validate embeddings
            if not isinstance(embeddings, list):
                raise EmbeddingGenerationError(
                    cleaned_queries[0],
                    f"Invalid batch embeddings type: {type(embeddings)}"
                )
            
            if len(embeddings) != len(cleaned_queries):
                raise EmbeddingGenerationError(
                    cleaned_queries[0],
                    f"Embedding count mismatch: expected {len(cleaned_queries)}, got {len(embeddings)}"
                )
            
            # Validate each embedding
            for i, embedding in enumerate(embeddings):
                if not isinstance(embedding, list):
                    raise EmbeddingGenerationError(
                        cleaned_queries[i],
                        f"Invalid embedding type at index {i}: {type(embedding)}"
                    )
                
                if len(embedding) != self.config.dimension:
                    raise EmbeddingGenerationError(
                        cleaned_queries[i],
                        f"Embedding dimension mismatch at index {i}: "
                        f"expected {self.config.dimension}, got {len(embedding)}"
                    )
            
            elapsed = (time.time() - start_time) * 1000
            avg_time = elapsed / len(cleaned_queries)
            logger.info(f"Generated {len(embeddings)} embeddings in {elapsed:.1f}ms "
                       f"(avg {avg_time:.1f}ms per query)")
            
            return embeddings
            
        except Exception as e:
            if isinstance(e, EmbeddingGenerationError):
                raise
            
            error_msg = f"Failed to generate batch embeddings: {str(e)}"
            logger.error(error_msg)
            raise EmbeddingGenerationError(
                cleaned_queries[0] if cleaned_queries else "",
                error_msg,
                original_error=e
            )
    
    def close(self) -> None:
        """
        Clean up resources.
        
        Releases any resources held by the embedding service.
        Safe to call multiple times.
        """
        if self._embed_model:
            logger.debug("Closing embedding service")
            self._embed_model = None
            self._initialized = False
    
    def __enter__(self):
        """Context manager entry."""
        self.initialize()
        return self
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """Context manager exit."""
        self.close()