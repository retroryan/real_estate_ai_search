# Test Configuration for Model Comparison
# This file specifies which embedding models to compare during evaluation

# Evaluation settings
evaluation:
  # Dataset to use: 'gold', 'generated', or 'both'
  dataset: gold
  
  # Number of top results to retrieve for each query
  top_k: 10
  
  # Whether to run evaluations in parallel
  parallel: true
  
  # Cache results to avoid re-computation
  use_cache: true
  cache_dir: common_embeddings/evaluate_results/cache

# Models to compare
models:
  - name: nomic-embed-text
    provider: ollama
    config:
      model: nomic-embed-text
      base_url: http://localhost:11434
    collection_name: wikipedia_ollama_nomic_embed_text_v1
    
  - name: mxbai-embed-large
    provider: ollama
    config:
      model: mxbai-embed-large
      base_url: http://localhost:11434
    collection_name: wikipedia_ollama_mxbai_embed_large_v1
    
  - name: text-embedding-3-small
    provider: openai
    config:
      model: text-embedding-3-small
      dimensions: 1536
    collection_name: wikipedia_openai_text_embedding_3_small_v1
    
  # - name: text-embedding-3-large
  #   provider: openai
  #   config:
  #     model: text-embedding-3-large
  #     dimensions: 3072
  #   collection_name: wikipedia_openai_text_embedding_3_large_v1
    
  # - name: gemini-embedding
  #   provider: gemini
  #   config:
  #     model: models/text-embedding-004
  #   collection_name: wikipedia_gemini_text_embedding_004_v1

# Comparison settings
comparison:
  # Metrics to focus on for comparison
  primary_metric: f1_score  # Options: precision, recall, f1_score, map, mrr
  
  # Statistical significance testing
  significance_test: true
  significance_level: 0.05
  
  # Performance thresholds
  thresholds:
    minimum_precision: 0.5
    minimum_recall: 0.4
    minimum_f1: 0.45

# Report generation
reporting:
  # Output directory for comparison reports
  output_dir: common_embeddings/evaluate_results/comparison
  
  # Report formats to generate
  formats:
    - html
    - json
    - markdown
  
  # Visualization settings
  visualizations:
    enabled: true
    include_charts:
      - metric_comparison_bar
      - category_heatmap
      - performance_radar
      - ranking_table
  
  # Include detailed per-query results
  include_detailed_results: true
  
  # Generate executive summary
  generate_summary: true

# Data preparation (if embeddings need to be created)
data_preparation:
  # Whether to create missing embeddings before evaluation
  auto_create_embeddings: false
  
  # Force recreation of all embeddings
  force_recreate: false
  
  # Articles and queries for evaluation
  articles_source: common_embeddings/evaluate_data/gold_articles.json
  queries_source: common_embeddings/evaluate_data/gold_queries.json